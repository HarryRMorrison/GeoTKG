{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6b9a55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Current Device: 0 NVIDIA GeForce RTX 4070 Ti\n"
     ]
    }
   ],
   "source": [
    "from scripts.model_utils import Utils\n",
    "import torch\n",
    "from transformers import RobertaForTokenClassification, RobertaTokenizerFast, TrainingArguments, Trainer\n",
    "from scripts.Reader import obtain_dataset, obtain_label_list\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "print(\"Current Device:\", torch.cuda.current_device(), torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7eb86e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model\n",
    "label_list, label2id, id2label = obtain_label_list(\"OzRock\")\n",
    "model_name = 'roberta-large'\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(model_name, add_prefix_space=True)\n",
    "model = RobertaForTokenClassification.from_pretrained(model_name, num_labels=len(label_list), label2id=label2id, id2label=id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38038161",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/Geo-NER\",\n",
    "    logging_dir=\"./logs/Geo-NER\",\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    num_train_epochs=1,\n",
    "    save_total_limit=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=5e-5,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4e4d22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c7c3a10f6c440f38151785fa46fb5d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/31942 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5514a2a7f864aeba2c91257eb5e4f44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils = Utils(tokenizer, label_list)\n",
    "tokenized_datasets = utils.tokenize_datasets(obtain_dataset(\"OzRock\", [\"train\", \"eval\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1eb45530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 31942\n",
       "    })\n",
       "    eval: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc67e586",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harry\\AppData\\Local\\Temp\\ipykernel_39816\\3749366058.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  geo_ner = Trainer(\n"
     ]
    }
   ],
   "source": [
    "geo_ner = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=utils.compute_metrics,\n",
    "    data_collator=utils.data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"eval\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "353225c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1997' max='1997' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1997/1997 2:37:47, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.269300</td>\n",
       "      <td>0.234656</td>\n",
       "      <td>0.569968</td>\n",
       "      <td>0.694251</td>\n",
       "      <td>0.626001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.127500</td>\n",
       "      <td>0.208862</td>\n",
       "      <td>0.659252</td>\n",
       "      <td>0.716221</td>\n",
       "      <td>0.686557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.091000</td>\n",
       "      <td>0.207840</td>\n",
       "      <td>0.726360</td>\n",
       "      <td>0.767851</td>\n",
       "      <td>0.746529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.069400</td>\n",
       "      <td>0.208985</td>\n",
       "      <td>0.734278</td>\n",
       "      <td>0.758880</td>\n",
       "      <td>0.746376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.062800</td>\n",
       "      <td>0.222484</td>\n",
       "      <td>0.690847</td>\n",
       "      <td>0.779385</td>\n",
       "      <td>0.732450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.052300</td>\n",
       "      <td>0.248187</td>\n",
       "      <td>0.723077</td>\n",
       "      <td>0.757232</td>\n",
       "      <td>0.739760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.047800</td>\n",
       "      <td>0.238044</td>\n",
       "      <td>0.757151</td>\n",
       "      <td>0.770597</td>\n",
       "      <td>0.763815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.047600</td>\n",
       "      <td>0.231846</td>\n",
       "      <td>0.744933</td>\n",
       "      <td>0.760344</td>\n",
       "      <td>0.752560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.037400</td>\n",
       "      <td>0.272573</td>\n",
       "      <td>0.750612</td>\n",
       "      <td>0.729586</td>\n",
       "      <td>0.739950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.032900</td>\n",
       "      <td>0.243237</td>\n",
       "      <td>0.749648</td>\n",
       "      <td>0.780117</td>\n",
       "      <td>0.764579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.033900</td>\n",
       "      <td>0.253488</td>\n",
       "      <td>0.764608</td>\n",
       "      <td>0.757049</td>\n",
       "      <td>0.760810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.250635</td>\n",
       "      <td>0.764800</td>\n",
       "      <td>0.787624</td>\n",
       "      <td>0.776044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.029800</td>\n",
       "      <td>0.273191</td>\n",
       "      <td>0.773588</td>\n",
       "      <td>0.762541</td>\n",
       "      <td>0.768025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.023600</td>\n",
       "      <td>0.253191</td>\n",
       "      <td>0.762748</td>\n",
       "      <td>0.796961</td>\n",
       "      <td>0.779479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.026000</td>\n",
       "      <td>0.276099</td>\n",
       "      <td>0.770822</td>\n",
       "      <td>0.770963</td>\n",
       "      <td>0.770892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.024300</td>\n",
       "      <td>0.260697</td>\n",
       "      <td>0.772983</td>\n",
       "      <td>0.780483</td>\n",
       "      <td>0.776715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.021400</td>\n",
       "      <td>0.256188</td>\n",
       "      <td>0.778080</td>\n",
       "      <td>0.785060</td>\n",
       "      <td>0.781555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.019400</td>\n",
       "      <td>0.267813</td>\n",
       "      <td>0.777253</td>\n",
       "      <td>0.783230</td>\n",
       "      <td>0.780230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.016600</td>\n",
       "      <td>0.274043</td>\n",
       "      <td>0.777677</td>\n",
       "      <td>0.787074</td>\n",
       "      <td>0.782348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1997, training_loss=0.05403430526353266, metrics={'train_runtime': 9468.3097, 'train_samples_per_second': 3.374, 'train_steps_per_second': 0.211, 'total_flos': 1.417626143339916e+16, 'train_loss': 0.05403430526353266, 'epoch': 1.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geo_ner.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "036fb79c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.2432372272014618,\n",
       " 'eval_precision': 0.7496481351161154,\n",
       " 'eval_recall': 0.7801171731966313,\n",
       " 'eval_f1': 0.7645792212452898,\n",
       " 'eval_runtime': 38.8285,\n",
       " 'eval_samples_per_second': 51.508,\n",
       " 'eval_steps_per_second': 1.623,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geo_ner.evaluate(tokenized_datasets[\"eval\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b62061ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_ner.save_model(\"./results/Geo-NER/final_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39b14634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and tokenizer from the folder\n",
    "model = RobertaForTokenClassification.from_pretrained(\"./results/Geo-NER/final_model/\")\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"./results/Geo-NER/final_model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f82890b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = [[0]*13]*13\n",
    "\n",
    "encodings=tokenizer(list(tokenized_datasets['eval']['tokens']), padding=True, truncation=True, return_tensors=\"pt\", is_split_into_words=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encodings)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d14ef510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[56, 6, 3, 14, 4, 9, 55, 0, 3, 10, 13, 0, 830],\n",
       " [9, 191, 6, 34, 2, 0, 3, 3, 5, 19, 7, 0, 1034],\n",
       " [2, 14, 6, 5, 0, 4, 3, 0, 8, 1, 6, 0, 333],\n",
       " [2, 76, 1, 210, 16, 26, 9, 0, 5, 99, 27, 0, 1409],\n",
       " [7, 5, 1, 24, 17, 8, 7, 0, 1, 16, 51, 0, 503],\n",
       " [9, 1, 2, 6, 2, 6, 8, 0, 0, 3, 4, 0, 169],\n",
       " [120, 2, 1, 9, 10, 12, 39, 0, 2, 4, 10, 0, 475],\n",
       " [0, 28, 0, 2, 0, 0, 1, 0, 0, 3, 1, 0, 56],\n",
       " [4, 12, 75, 1, 0, 0, 1, 2, 8, 3, 2, 0, 193],\n",
       " [2, 38, 0, 139, 3, 6, 2, 0, 2, 48, 4, 0, 512],\n",
       " [29, 8, 0, 32, 158, 12, 8, 0, 1, 15, 70, 0, 447],\n",
       " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2],\n",
       " [753, 1586, 201, 1713, 346, 217, 427, 27, 197, 777, 732, 0, 36257]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = [[0 for _ in range(13)] for _ in range(13)]\n",
    "for preds, actua in zip(predictions,tokenized_datasets['eval']['ner_tags']):\n",
    "    preds = preds[:len(actua)]\n",
    "    for j, predicted_label in enumerate(preds):\n",
    "        conf[actua[j]][predicted_label] += 1  \n",
    "\n",
    "conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ff23e89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0558 0.0564 0.0561\n",
      "0.1455 0.0971 0.1165\n",
      "0.0157 0.0203 0.0177\n",
      "0.1117 0.0959 0.1032\n",
      "0.0266 0.0305 0.0284\n",
      "0.0286 0.0199 0.0235\n",
      "0.057 0.0693 0.0626\n",
      "0.0 0.0 0\n",
      "0.0266 0.0345 0.03\n",
      "0.0635 0.0481 0.0547\n",
      "0.0897 0.0755 0.082\n",
      "0.0 0 0\n",
      "0.8386 0.8588 0.8486\n"
     ]
    }
   ],
   "source": [
    "def metr(conf, index):\n",
    "    TP = conf[index][index]\n",
    "    FN = sum(conf[index])-TP\n",
    "    FP = sum(uh[index] for uh in conf)-TP\n",
    "    try:\n",
    "        recall = TP/(TP+FN)\n",
    "    except ZeroDivisionError:\n",
    "        recall = 0\n",
    "    try:\n",
    "        prec = TP/(TP+FP)\n",
    "    except ZeroDivisionError:\n",
    "        prec=0\n",
    "    try:\n",
    "        f1 = (2*recall*prec)/(recall+prec)\n",
    "    except ZeroDivisionError:\n",
    "        f1=0\n",
    "    return recall, prec, f1\n",
    "\n",
    "for i in range(13):\n",
    "    r,p,f = metr(conf,i)\n",
    "    print(round(r,4),round(p,4),round(f,4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387c5052",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GeoTKG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
